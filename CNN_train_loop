###Model training

import torch
import matplotlib.pyplot as plt
from torch import nn
import torchvision
from torchvision import transforms
from pathlib import Path
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import timeit #for timing execution time
import numpy as np
from typing import List, Tuple, Dict, Optional
import math

import torchinfo
from torchinfo import summary

import utils, engine, EffNet_Implementation, model_builder, Effnet_v2, ignite_engine

#set some vars
BATCH_SIZE = 64
RANDOM_SEED = 3
EPOCHS = 10 

###Set up device agnostic code. 
device = "cuda" if t.cuda.is_available() else "cpu"
print(device)

###Getting the training/testing data, as well as data augmentation.

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),

])

validation_transform = transforms.Compose([
    transforms.ToTensor(),
])

train_data = torchvision.datasets.CIFAR10(
    root = 'CIFAR10/train',
    train = True,
    download = True,
    transform = train_transform
)

validation_data = torchvision.datasets.CIFAR10(
    root = 'CIFAR10/validation',
    train = False,
    download = True,
    transform = validation_transform
)

classes = train_data.classes

train_dataloader  = DataLoader(train_data,
                               batch_size = BATCH_SIZE,
                               shuffle = True)

validation_dataloader = DataLoader(validation_data,
                                   batch_size = BATCH_SIZE,
                                   shuffle = True) #false should be fine

### Setting up the model
args_dict = [ #num_repeat, kernel_size, stride,  expand_ratio, se_reduction_ratio, id_skip, input_channels, output_channels
        [1, 3, [1,1],  1, 0.25, True,32,16], 
        [2, 3, [2,2],  6, 0.25, True,16,32], 
        [2, 5, [2,2],  6, 0.25, True,32,64], 
        [3, 3, [2,2],  6, 0.25, True,64,128], 
        [3, 5, [1,1],  6, 0.25, True,128, 256], 
        # [4, 5, [2,2],  6, 0.25, True,112,192], 
        # [1, 3, [1,1],  6, 0.25, True,192,320], #3 #deeper layers require mor eprecessing, so its commented here.  
    ]

global_params, blocks_params = Effnet_v2.set_default_params(color_channels = 1, width_coeff = 0.5, depth_coeff = 0.25, depth_divisor = 8, dropout_rate = 0.2, args_dict = args_dict)
model = Effnet_v2.EfficientNet(global_params=global_params, blocks_params = blocks_params).to(device)

optimizer = t.optim.RMSprop(params = model.parameters(), lr = 1e-4)
loss_fn = nn.CrossEntropyLoss()
writer = utils.create_writer(experiment_name = 'EffNet', model_name = 'EffNet')

#get a summary of the model's architecture
print(utils.get_model_summary(model = model, input_size = (BATCH_SIZE, 3, 32, 32), device = device)) #change input size to fit with data size. Different dims may encounter some issues. 

#training func
results = ignite_engine.engine(
    model = model,
    train_dataloader = train_dataloader,
    validation_dataloader = validation_dataloader,
    optimizer = optimizer,
    loss_fn = loss_fn,
    epochs = EPOCHS,
    device = device,
    lr_scheduler =  None,
    log_interval = 1000
)
